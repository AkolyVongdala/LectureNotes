{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Principles\n",
    "Testing is the process by which you exercise your code to determine if it performs as expected. The code you are testing is referred to as the **code under test**. \n",
    "\n",
    "There are two parts to writing tests.\n",
    "1. invoking the code under test so that it is exercised in a particular way;\n",
    "1. evaluating the results of executing code under test to determine if it behaved as expected.\n",
    "\n",
    "The collection of tests performed are referred to as the **test cases**. The fraction of the code under test that is executed as a result of running the test cases is referred to as **test coverage**.\n",
    "\n",
    "For dynamical languages such as Python, it's extremely important to have a high test coverage. In fact, you should try to get 100% coverage. This is because little checking is done when the source code is read by the Python interpreter. For example, the code under test might contain a line that has a function that is undefined. This would not be detected until that line of code is executed.\n",
    "\n",
    "Test cases can be of several types. Below are listed some common classifications of test cases.\n",
    "- *Smoke test*. This is an invocation of the code under test to see if there is an unexpected exception. It's useful as a starting point, but this doesn't tell you anything about the correctness of the results of a computation.\n",
    "- *One-shot test*. In this case, you call the code under test with arguments for which you know the expected result.\n",
    "- *Edge test*. The code under test is invoked with arguments that should cause an exception, and you evaluate if the expected exception occurrs.\n",
    "- *Pattern test* - Based on your knowledge of the *calculation* (not implementation) of the code under test, you construct a suite of test cases for which the results are known or there are known patterns in these results that are used to evaluate the results returned.\n",
    "\n",
    "Another principle of testing is to limit what is done in a single test case. Generally, a test case should focus on one use of one function. Sometimes, this is a challenge since the function being tested may call other functions that you are testing. This means that bugs in the called functions may cause failures in the tests of the calling functions. Often, you sort this out by knowing the structure of the code and focusing first on failures in lower level tests. In other situations, you may use more advanced techniques called *mocking*. A discussion of mocking is beyond the scope of this course.\n",
    "\n",
    "A best practice is to develop your tests while you are developing your code. Indeed, one school of thought in software engineering, called **test-driven development**, advocates that you write the tests *before* you implement the code under test so that the test cases become a kind of specification for what the code under test should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Test Cases\n",
    "This section presents examples of test cases. The code under test is the calculation of entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of a set of probabilities\n",
    "$$\n",
    "H = -\\sum_i p_i \\log(p_i)\n",
    "$$\n",
    "The calculation expects that the $\\sum_i p_i = 1$. So, this is something that our implementation should check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Under Test\n",
    "def entropy(ps):\n",
    "    if not np.isclose(sum(ps), 1):\n",
    "        raise ValueError(\"Probabilities must sum to 1.\")\n",
    "    items = ps * np.log(ps)\n",
    "    return -np.sum(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that all of the probability of a distribution is at one point. An example of this is a coin with two heads. Whenever you flip it, you always get heads. That is, the probability of a head is 1.\n",
    "\n",
    "What is the entropy of such a distribution? From the calculation above, we see that the entropy should be $log(1)$, which is 0. This means that we have a test case where we know the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worked!\n"
     ]
    }
   ],
   "source": [
    "# One-shot test.\n",
    "if entropy([1.0]) != 0:\n",
    "    print (\"Bad result!\")\n",
    "else:\n",
    "    print(\"Worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One edge test of interest is to provide an input that is *not* a distribution in that probabilities don't sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worked!\n"
     ]
    }
   ],
   "source": [
    "# Edge test.\n",
    "try:\n",
    "  entropy([0.1, 0.5])\n",
    "  print (\"Bad result.\")\n",
    "except ValueError:\n",
    "  print (\"Worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a pattern test. Examining the structure of the calculation of $H$, we consider a situation in which there are $n$ equal probabilities. That is, $p_i = \\frac{1}{n}$.\n",
    "$$\n",
    "H = -\\sum_{i=1}^{n} p_i \\log(p_i) \n",
    "= -\\sum_{i=1}^{n} \\frac{1}{n} \\log(\\frac{1}{n}) \n",
    "= n (-\\frac{1}{n} \\log(\\frac{1}{n}) )\n",
    "= -\\log(\\frac{1}{n})\n",
    "$$\n",
    "For example, entropy([0.5, 0.5]) should be $-log(0.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worked!\n"
     ]
    }
   ],
   "source": [
    "# Pattern test\n",
    "def test_equal_probabilities(n):\n",
    "    prob = 1.0/n\n",
    "    ps = np.repeat(prob , n)\n",
    "    if entropy(ps) != -np.log(prob):\n",
    "        import pdb; pdb.set_trace()\n",
    "        print (\"Bad result.\")\n",
    "    else:\n",
    "        print(\"Worked!\")\n",
    "        \n",
    "# Run a test\n",
    "test_equal_probabilities(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that there are many, many cases to test. So far, we've been writing special codes for each test case. We can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unittest Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several reasons to use a test infrastructure:\n",
    "- If you have a lot test cases, you'll end up writing a lot less code.\n",
    "- The infrastructure provides a uniform way to report test failures, and to report the failures of many tests, rather than just one at a time.\n",
    "- A test infrastructure can tell you about coverage so you know what tests to add.\n",
    "\n",
    "We'll be using the `unittest` framework. This is a separate Python package. Using this infrastructure, requires the following:\n",
    "1. import the unittest module\n",
    "1. define a class that inherits from unittest.TestCase\n",
    "1. write methods that run the code to be tested and check the outcomes.\n",
    "\n",
    "The last item has two subparts. First, we must identify which methods in the class inheriting from unittest.TestCase are tests. You indicate that a method is to be run as a test by having the method name begin with \"test\".\n",
    "\n",
    "Second, the \"test methods\" should communicate with the infrastructure the results of evaluating output from the code under test. This is done by using `assert` statements. For example, `self.assertEqual` takes two arguments. If these are objects for which `==` returns `True`, then the test passes. Otherwise, the test fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class UnitTests(unittest.TestCase):\n",
    "\n",
    "    # Each method in the class to execute a test\n",
    "    def test_upper(self):\n",
    "        self.assertEqual(1, 1)\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertEqual(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running unittests inside Jupyter requires some special code.\n",
    "# This code is encapsulated inthe function below. When you create\n",
    "# files containing unittests, it will look simpler.\n",
    "def test(test_class=UnitTests):\n",
    "    # Convenience function to run tests.\n",
    "    # test_class is the class containing the tests.\n",
    "    suite = unittest.TestLoader().loadTestsFromModule(test_class())\n",
    "    unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F.\n",
      "======================================================================\n",
      "FAIL: test_isupper (__main__.UnitTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-091b0079f427>\", line 11, in test_isupper\n",
      "    self.assertEqual(1, 2)\n",
      "AssertionError: 1 != 2\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.009s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "# The function test runs the class UnitTests.\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the first test passes, but the second test fails.\n",
    "\n",
    "Below, we test the `entropy` function using the unittest infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class EntropyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_shot(self):\n",
    "        self.assertEqual(entropy([1.0]), 0.0)\n",
    "        \n",
    "test(test_class=EntropyTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there's some setup, the tests are much easier than what we did before\n",
    "```\n",
    "ps = 1.0  # Should have an entropy of 0\n",
    "if entropy(ps) != 0:\n",
    "    print (\"Got a bad result.\")\n",
    "```\n",
    "\n",
    "Now we can add LOTS of tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F.\n",
      "======================================================================\n",
      "FAIL: test_equal_probability (__main__.EntropyTest)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-10-a89793b0b452>\", line 17, in test_equal_probability\n",
      "    self._test_equal_probability(200)\n",
      "  File \"<ipython-input-10-a89793b0b452>\", line 12, in _test_equal_probability\n",
      "    self.assertEqual(entropy(ps), -np.log(prob))\n",
      "AssertionError: 5.2983173665480372 != 5.2983173665480363\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.012s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class EntropyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_shot(self):\n",
    "        self.assertEqual(entropy([1.0]), 0.0)\n",
    "        \n",
    "    def _test_equal_probability(self, n):\n",
    "        prob = 1.0/n\n",
    "        ps = np.repeat(prob , n)\n",
    "        self.assertEqual(entropy(ps), -np.log(prob))\n",
    "        \n",
    "    def test_equal_probability(self):\n",
    "        self._test_equal_probability(2)\n",
    "        self._test_equal_probability(20)\n",
    "        self._test_equal_probability(200)\n",
    "        \n",
    "test(test_class=EntropyTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did this test fail? How do we deal with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class EntropyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_shot(self):\n",
    "        self.assertEqual(entropy([1.0]), 0.0)\n",
    "        \n",
    "    def _test_equal_probability(self, n):\n",
    "        prob = 1.0/n\n",
    "        ps = np.repeat(prob , n)\n",
    "        self.assertTrue(np.isclose(entropy(ps), -np.log(prob)))\n",
    "        \n",
    "    def test_equal_probability(self):\n",
    "        self._test_equal_probability(2)\n",
    "        self._test_equal_probability(20)\n",
    "        self._test_equal_probability(200)\n",
    "        \n",
    "test(test_class=EntropyTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing For Exceptions\n",
    "\n",
    "Testing edge cases often involves handling exceptions. One approach is to code this directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.007s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class EntropyTest(unittest.TestCase):\n",
    "        \n",
    "    def test_invalid_probability(self):\n",
    "        try:\n",
    "            entropy([0.1, 0.5])\n",
    "            self.assertTrue(False)\n",
    "        except ValueError:\n",
    "            self.assertTrue(True)\n",
    "        \n",
    "test(test_class=EntropyTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unittest` provides help with testing exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "# Define a class in which the tests will run\n",
    "class EntropyTest(unittest.TestCase):\n",
    "        \n",
    "    def test_invalid_probability(self):\n",
    "        with self.assertRaises(ValueError):\n",
    "            entropy([0.1, 0.5])\n",
    "        \n",
    "test(test_class=EntropyTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Files\n",
    "Although I presented the elements of `unittest` in a notebook. your tests should be in a file. If the name of module with the code under test is `foo.py`, then the name of the test file should be `test_foo.py`.\n",
    "\n",
    "The structure of the test file will be very similar to cells above. You will import `unittest`. You must also import the module with the code under test. Take a look at `test_prime.py` in this directory to see an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "**Question**: What tests would you write for a plotting function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Debug prime.py\n",
    "- Write a one-shot, edge test, and pattern test for prime.py.\n",
    "- Try using nosetests to get coverage information (`nosetests --with-coverage test_prime.py`). You may have to install the coverage module (look at https://stackoverflow.com/questions/14488601/how-to-fix-python-nose-coverage-not-available-unable-to-import-coverage-module)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
